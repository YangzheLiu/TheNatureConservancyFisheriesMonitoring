{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Reshape, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from __future__ import division, print_function\n",
    "from collections import Counter\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imshow\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((3,1,1))\n",
    "def vgg_preprocess(x):\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1]\n",
    "\n",
    "def VGG_16(size=(224, 224), weights_path='data/vgg16_bn_conv.txt'):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(vgg_preprocess, input_shape=(3,)+size))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = (360, 270)\n",
    "model = VGG_16(size=size)\n",
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('vgg16_group_order_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bath_size = 8\n",
    "random_seed = np.random.random_integers(0, 100000)\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "            'data/test2/',\n",
    "            target_size=size,\n",
    "            batch_size=bath_size,\n",
    "            shuffle = False,\n",
    "            seed = random_seed,\n",
    "            classes = None,\n",
    "            class_mode = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_test_samples = len(glob.glob('data/test2/*/*.jpg'))+len(glob.glob('data/test2/*/*/*.jpg'))\n",
    "prediction = model.predict_generator(test_generator, nb_test_samples)\n",
    "test_image_list = test_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ship_dict={}\n",
    "for i in range(0,len(test_image_list)):\n",
    "    temp_index=0\n",
    "    for j in range(0,prediction.shape[1]):\n",
    "        if prediction[i][j]==max(prediction[i]):\n",
    "            temp_index=j+1\n",
    "    train_ship_dict[test_image_list[i]]=temp_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_pic in train_ship_dict:\n",
    "   os.rename('data/test2 _tem/'+train_pic, 'data/test2 _tem/' +str(train_ship_dict[train_pic])+'/'+train_pic.split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_dict={\n",
    "1:\n",
    "(0.082, 0.347, 0.586, 1),\n",
    "2:\n",
    "(0.375, 0, 0.555, 0.306),\n",
    "3:\n",
    "(0.328, 0.642, 0.563, 1),\n",
    "4:\n",
    "(0.078, 0, 0.781, 0.875),\n",
    "5:\n",
    "(0.156, 0, 0.75, 0.867),\n",
    "6:\n",
    "(0.375, 0.319, 0.797, 1),\n",
    "7:\n",
    "((0,0,0.563,1),    (0.437,0,1,1,)),\n",
    "8:\n",
    "(0.356, 0, 0.784, 0.703),\n",
    "9:\n",
    "(0, 0, 0.503, 0.895),\n",
    "10:\n",
    "(0.437,0,1,1),\n",
    "11:\n",
    "(0,0,0.563,1),\n",
    "12:\n",
    "(0.437,0,1,1),\n",
    "13:\n",
    "(0,0,1,1),\n",
    "14:\n",
    "(0.436, 0, 0.864, 0.657),\n",
    "15:\n",
    "(0,0,1,1),\n",
    "16:\n",
    "(0.336, 0.153, 0.766, 0.861)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_pic in train_ship_dict:\n",
    "    if train_ship_dict[train_pic] != 7:\n",
    "        img = PIL.Image.open('data/test/'+train_pic)  \n",
    "        croped = img.crop((int(img.width*crop_dict[train_ship_dict[train_pic]][0]),int(img.height*crop_dict[train_ship_dict[train_pic]][1]),int(img.width*crop_dict[train_ship_dict[train_pic]][2]),int(img.height*crop_dict[train_ship_dict[train_pic]][3])))\n",
    "        croped.save('data/test_crop/'+train_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_pic in train_ship_dict:\n",
    "    if train_ship_dict[train_pic] == 7:\n",
    "        img = PIL.Image.open('data/test/'+train_pic)  \n",
    "        croped = img.crop((int(img.width*crop_dict[train_ship_dict[train_pic]][0][0]),int(img.height*crop_dict[train_ship_dict[train_pic]][0][1]),int(img.width*crop_dict[train_ship_dict[train_pic]][0][2]),int(img.height*crop_dict[train_ship_dict[train_pic]][0][3])))\n",
    "        croped.save('data/test_ship7_train/'+train_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_pic in train_ship_dict:\n",
    "    if train_ship_dict[train_pic] == 7:\n",
    "        img = PIL.Image.open('data/test/'+train_pic)  \n",
    "        croped = img.crop((int(img.width*crop_dict[train_ship_dict[train_pic]][1][0]),int(img.height*crop_dict[train_ship_dict[train_pic]][1][1]),int(img.width*crop_dict[train_ship_dict[train_pic]][1][2]),int(img.height*crop_dict[train_ship_dict[train_pic]][1][3])))\n",
    "        croped.save('data/test_ship7_train_2/'+train_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_glob=glob.glob('data/ship7_train_2/*/*')\n",
    "for img_path in img_glob:\n",
    "    img = PIL.Image.open(img_path)\n",
    "    img.save('data/final_train/'+img_path.split('/')[2]+'/'+str(random.randint(0,9999999999999999))+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# img_glob=glob.glob('data/internetFish_back/*/*')\n",
    "for img_path in img_glob:\n",
    "    os.rename(img_path, 'data/final_train/'+img_path.split('/')[2]+'/'+str(random.randint(0,9999999999999999))+'.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
